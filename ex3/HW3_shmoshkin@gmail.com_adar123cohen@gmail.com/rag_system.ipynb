{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hugging_face_1bLLamaInstruct = \"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\"\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(hugging_face_1bLLamaInstruct)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y openjdk-21-jdk\n",
        "!update-alternatives --install /usr/bin/java java /usr/lib/jvm/java-21-openjdk-amd64/bin/java 1\n",
        "!update-alternatives --install /usr/bin/javac javac /usr/lib/jvm/java-21-openjdk-amd64/bin/javac 1\n",
        "!update-alternatives --set java /usr/lib/jvm/java-21-openjdk-amd64/bin/java\n",
        "!update-alternatives --set javac /usr/lib/jvm/java-21-openjdk-amd64/bin/javac\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install faiss-cpu --no-cache\n",
        "!pip install pyserini==0.36.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize searcher with Wikipedia KILT index\n",
        "from pyserini.search import SimpleSearcher\n",
        "searcher = SimpleSearcher.from_prebuilt_index('wikipedia-kilt-doc')\n",
        "\n",
        "# Display index statistics\n",
        "from pyserini.index.lucene import IndexReader\n",
        "index_reader = IndexReader.from_prebuilt_index('wikipedia-kilt-doc')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(index_reader.stats())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read the CSV files\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "df_train = pd.read_csv(\"./train.csv\", converters={\"answers\": json.loads})\n",
        "df_test = pd.read_csv(\"./test.csv\")\n",
        "\n",
        "print(f\"\\n‚úÖ Successfully loaded:\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train.head()\n",
        "df_test.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# IMPROVED Configuration - Optimized to reduce \"I don't know\" answers\n",
        "# ============================================================================\n",
        "\n",
        "# Data paths (adjust if your files are in a different location)\n",
        "TRAIN_CSV = \"./train.csv\"\n",
        "TEST_CSV = \"./test.csv\"\n",
        "PREDICTIONS_CSV = \"./predictions.csv\"\n",
        "CHECKPOINT_FILE = \"./checkpoint.json\"\n",
        "\n",
        "# Retrieval parameters - OPTIMIZED\n",
        "K = 30  # Increased for better recall - more passages = better chance of finding answer\n",
        "RETRIEVAL_METHOD = \"qld\"  # \"qld\" (primary), \"bm25\" (optional), or \"rrf\" (fusion - recommended)\n",
        "QLD_MU = 1000  # Standard value - tune between 500-2000\n",
        "BM25_K1 = 1.2  # BM25 k1 parameter (standard: 1.2)\n",
        "BM25_B = 0.75  # BM25 b parameter (standard: 0.75)\n",
        "CONTEXT_LENGTH = 800  # NO TRUNCATION! Keep full passages\n",
        "\n",
        "# Advanced retrieval options (for RRF)\n",
        "USE_RRF = False  # Set to True to use RRF even if RETRIEVAL_METHOD != \"rrf\"\n",
        "RRF_K = 60  # Number of docs to retrieve from each method for RRF\n",
        "RRF_FINAL_K = 30  # Final number of docs after RRF fusion\n",
        "\n",
        "# LLM parameters - IMPROVED\n",
        "MAX_NEW_TOKENS = 256  # DECREASED from 256 - shorter, more focused answers\n",
        "TEMPERATURE = 0.6  # DECREASED from 0.6 - more deterministic, less random\n",
        "TOP_P = 0.9  # Slightly increased for better quality\n",
        "DO_SAMPLE = True\n",
        "\n",
        "# Processing\n",
        "SAVE_CHECKPOINT_EVERY = 50  # Save checkpoint every N questions\n",
        "RESUME_FROM_CHECKPOINT = True  # Resume if checkpoint exists\n",
        "\n",
        "# Debug options\n",
        "DEBUG_PRINT_CONTEXTS = True  # When True, print retrieved passages before sending to LLM\n",
        "\n",
        "print(\"‚úÖ OPTIMIZED Configuration loaded\")\n",
        "print(f\"Key changes:\")\n",
        "print(f\"  - K: 10 ‚Üí {K} (more passages for better recall)\")\n",
        "print(f\"  - RETRIEVAL_METHOD: {RETRIEVAL_METHOD} (RRF combines QLD + BM25)\")\n",
        "print(f\"  - QLD_MU: {QLD_MU} (standard value)\")\n",
        "print(f\"  - BM25: k1={BM25_K1}, b={BM25_B} (standard values)\")\n",
        "print(f\"  - CONTEXT_LENGTH: {CONTEXT_LENGTH} (no truncation)\")\n",
        "print(f\"  - TEMPERATURE: {TEMPERATURE} (more deterministic)\")\n",
        "print(f\"  - MAX_NEW_TOKENS: {MAX_NEW_TOKENS} (shorter answers)\")\n",
        "if RETRIEVAL_METHOD == \"rrf\":\n",
        "    print(f\"  - RRF: Retrieving {RRF_K} docs from each method, fusing to {RRF_FINAL_K}\")\n",
        "print(f\"\\nRetrieval method: {RETRIEVAL_METHOD}, k={K}, QLD_mu={QLD_MU}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import functions from rag_system.py\n",
        "# Make sure rag_system.py is uploaded to your Colab environment\n",
        "import sys\n",
        "sys.path.append('/content/wet')\n",
        "from rag_system import (\n",
        "    get_context_qld,\n",
        "    get_context_bm25,\n",
        "    reciprocal_rank_fusion,\n",
        "    create_message,\n",
        "    extract_answer,\n",
        "    print_contexts,\n",
        "    normalize_answer,\n",
        "    f1_score,\n",
        "    metric_max_over_ground_truths,\n",
        "    score,\n",
        "    load_llm_pipeline\n",
        ")\n",
        "from pathlib import Path\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Create a simple config class for compatibility with rag_system.py functions\n",
        "class SimpleConfig:\n",
        "    \"\"\"Simple config class that uses notebook variables\"\"\"\n",
        "    def __init__(self):\n",
        "        self.QLD_MU = QLD_MU\n",
        "        self.BM25_K1 = BM25_K1\n",
        "        self.BM25_B = BM25_B\n",
        "        self.CONTEXT_LENGTH = CONTEXT_LENGTH\n",
        "        self.CONTEXT_TOP_N = CONTEXT_TOP_N\n",
        "        self.CHECKPOINT_FILE = Path(CHECKPOINT_FILE)\n",
        "        self.RESUME_FROM_CHECKPOINT = RESUME_FROM_CHECKPOINT\n",
        "\n",
        "# Wrapper function for get_context that uses notebook config\n",
        "def get_context_wrapper(searcher, query, k, retrieval_method):\n",
        "    \"\"\"Wrapper for get_context that uses notebook config variables\"\"\"\n",
        "    # Reciprocal Rank Fusion: combine QLD and BM25\n",
        "    if retrieval_method == \"rrf\" or USE_RRF:\n",
        "        # Retrieve from both methods (need to switch searcher settings)\n",
        "        searcher.set_qld(mu=QLD_MU)\n",
        "        qld_hits = get_context_qld(searcher, query, RRF_K, mu=QLD_MU)\n",
        "        \n",
        "        searcher.set_bm25(k1=BM25_K1, b=BM25_B)\n",
        "        bm25_hits = get_context_bm25(searcher, query, RRF_K, k1=BM25_K1, b=BM25_B)\n",
        "        \n",
        "        # Fuse results using RRF\n",
        "        hits = reciprocal_rank_fusion([qld_hits, bm25_hits], k=RRF_K, final_k=RRF_FINAL_K)\n",
        "        \n",
        "    elif retrieval_method == \"qld\":\n",
        "        searcher.set_qld(mu=QLD_MU)\n",
        "        hits = get_context_qld(searcher, query, k, mu=QLD_MU)\n",
        "    elif retrieval_method == \"bm25\":\n",
        "        searcher.set_bm25(k1=BM25_K1, b=BM25_B)\n",
        "        hits = get_context_bm25(searcher, query, k, k1=BM25_K1, b=BM25_B)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown retrieval method: {retrieval_method}. Use 'qld', 'bm25', or 'rrf'\")\n",
        "    \n",
        "    # Extract passage text\n",
        "    contexts = []\n",
        "    for hit in hits:\n",
        "        try:\n",
        "            doc = searcher.doc(hit.docid)\n",
        "            raw_json = doc.raw()\n",
        "            data = json.loads(raw_json)\n",
        "            contents = data['contents']\n",
        "            \n",
        "            # Clean and truncate if needed\n",
        "            content = contents.replace('\\n', ' ')\n",
        "            if CONTEXT_LENGTH > 0 and len(content) > CONTEXT_LENGTH:\n",
        "                content = content[:CONTEXT_LENGTH] + \"...\"\n",
        "            \n",
        "            contexts.append(content)\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not retrieve document {hit.docid}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    # Limit to top N contexts (if configured)\n",
        "    if CONTEXT_TOP_N > 0 and len(contexts) > CONTEXT_TOP_N:\n",
        "        contexts = contexts[:CONTEXT_TOP_N]\n",
        "    \n",
        "    # Print contexts for debugging if enabled\n",
        "    # Use tqdm.write() to ensure output appears above progress bar\n",
        "    if DEBUG_PRINT_CONTEXTS:\n",
        "        print_contexts(query, contexts, use_tqdm=True)\n",
        "    \n",
        "    return contexts\n",
        "\n",
        "# Wrapper functions for checkpointing that use notebook config\n",
        "def save_checkpoint_wrapper(predictions, processed_ids):\n",
        "    \"\"\"Save checkpoint using notebook config\"\"\"\n",
        "    checkpoint = {\n",
        "        \"predictions\": predictions,\n",
        "        \"processed_ids\": processed_ids\n",
        "    }\n",
        "    with open(CHECKPOINT_FILE, 'w') as f:\n",
        "        json.dump(checkpoint, f, indent=2)\n",
        "    print(f\"Checkpoint saved: {len(predictions)} predictions\")\n",
        "\n",
        "def load_checkpoint_wrapper():\n",
        "    \"\"\"Load checkpoint using notebook config\"\"\"\n",
        "    checkpoint_path = Path(CHECKPOINT_FILE)\n",
        "    if checkpoint_path.exists() and RESUME_FROM_CHECKPOINT:\n",
        "        try:\n",
        "            with open(CHECKPOINT_FILE, 'r') as f:\n",
        "                checkpoint = json.load(f)\n",
        "            print(f\"Checkpoint loaded: {len(checkpoint['predictions'])} predictions\")\n",
        "            return checkpoint[\"predictions\"], set(checkpoint[\"processed_ids\"])\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading checkpoint: {e}\")\n",
        "    return {}, set()\n",
        "\n",
        "print(\"‚úÖ RAG system functions imported\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup Retrieval and LLM\n",
        "\n",
        "Configure the searcher and get terminators for LLM generation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set retrieval method\n",
        "# Note: For RRF, we don't set a single method - we'll use both QLD and BM25\n",
        "if RETRIEVAL_METHOD == \"qld\":\n",
        "    searcher.set_qld(mu=QLD_MU)\n",
        "elif RETRIEVAL_METHOD == \"bm25\":\n",
        "    searcher.set_bm25(k1=BM25_K1, b=BM25_B)\n",
        "elif RETRIEVAL_METHOD == \"rrf\":\n",
        "    # RRF uses both methods, so we'll set both (will switch during retrieval)\n",
        "    # Default to QLD for now, will switch to BM25 when needed\n",
        "    searcher.set_qld(mu=QLD_MU)\n",
        "    print(\"‚úÖ RRF mode: Will use both QLD and BM25 during retrieval\")\n",
        "\n",
        "# Load LLM pipeline\n",
        "print(\"Loading LLM pipeline...\")\n",
        "pipeline = load_llm_pipeline()\n",
        "print(\"‚úÖ LLM pipeline loaded\")\n",
        "\n",
        "# Get terminators for LLM generation\n",
        "terminators = [\n",
        "    pipeline.tokenizer.eos_token_id,\n",
        "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n",
        "\n",
        "print(f\"‚úÖ Searcher configured with method: {RETRIEVAL_METHOD}\")\n",
        "if RETRIEVAL_METHOD == \"rrf\":\n",
        "    print(f\"   - RRF will retrieve {RRF_K} docs from each method, fuse to {RRF_FINAL_K}\")\n",
        "print(\"‚úÖ LLM terminators ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate on Training Set (Run First!)\n",
        "\n",
        "Evaluate the system on the training set to compute F1 score and compare with baseline.\n",
        "**Only proceed to test data after you're satisfied with training results.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CLEANUP: Delete old checkpoint and prediction files before training evaluation\n",
        "# ============================================================================\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# Files to delete\n",
        "files_to_delete = [\n",
        "    Path(CHECKPOINT_FILE),  # Main checkpoint file\n",
        "    Path(CHECKPOINT_FILE).parent / \"train_checkpoint.json\",  # Training checkpoint\n",
        "    Path(PREDICTIONS_CSV),  # Predictions CSV file\n",
        "]\n",
        "\n",
        "print(\"üßπ Cleaning up old checkpoint and prediction files...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "deleted_count = 0\n",
        "for file_path in files_to_delete:\n",
        "    if file_path.exists():\n",
        "        try:\n",
        "            file_path.unlink()\n",
        "            print(f\"  ‚úÖ Deleted: {file_path}\")\n",
        "            deleted_count += 1\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö†Ô∏è  Error deleting {file_path}: {e}\")\n",
        "    else:\n",
        "        print(f\"  ‚ÑπÔ∏è  Not found (already clean): {file_path}\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "if deleted_count > 0:\n",
        "    print(f\"‚úÖ Cleanup complete. Deleted {deleted_count} file(s). Ready for fresh training evaluation.\")\n",
        "else:\n",
        "    print(\"‚úÖ Cleanup complete. No files to delete. Ready for fresh training evaluation.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process training questions for evaluation\n",
        "from typing import Dict\n",
        "from pathlib import Path\n",
        "\n",
        "# Limit to first 1000 questions for faster evaluation\n",
        "TRAIN_SAMPLE_SIZE = 1000\n",
        "df_train_sample = df_train.head(TRAIN_SAMPLE_SIZE).copy()\n",
        "\n",
        "# Dictionary to store predictions: {question_id: predicted_answer_string}\n",
        "predictions_train: Dict[int, str] = {}\n",
        "\n",
        "# Load checkpoint if exists (for resuming interrupted evaluation)\n",
        "train_checkpoint_file = Path(CHECKPOINT_FILE).parent / \"train_checkpoint.json\"\n",
        "processed_train_ids = set()\n",
        "\n",
        "if RESUME_FROM_CHECKPOINT and train_checkpoint_file.exists():\n",
        "    try:\n",
        "        with open(train_checkpoint_file, 'r') as f:\n",
        "            checkpoint = json.load(f)\n",
        "            predictions_train = {int(k): v for k, v in checkpoint.get(\"predictions\", {}).items()}\n",
        "            processed_train_ids = set(checkpoint.get(\"processed_ids\", []))\n",
        "            print(f\"‚úÖ Resumed from checkpoint: {len(predictions_train)}/{len(df_train_sample)} predictions loaded\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Error loading checkpoint: {e}\")\n",
        "        print(\"   Starting fresh evaluation...\")\n",
        "        predictions_train = {}\n",
        "        processed_train_ids = set()\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Evaluating on Training Set (First 1000 Questions)\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Retrieval method: {RETRIEVAL_METHOD}, k={K}\")\n",
        "print(f\"Total training questions in dataset: {len(df_train)}\")\n",
        "print(f\"Processing sample size: {len(df_train_sample)} questions\")\n",
        "print(f\"Already processed: {len(processed_train_ids)}\")\n",
        "print(f\"Remaining: {len(df_train_sample) - len(processed_train_ids)}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for index, row in tqdm(df_train_sample.iterrows(), total=len(df_train_sample), desc=\"Processing training questions\"):\n",
        "    question = row['question']\n",
        "    qid = row['id']\n",
        "    \n",
        "    # Skip if already processed (when resuming)\n",
        "    if qid in processed_train_ids:\n",
        "        continue\n",
        "    \n",
        "    try:\n",
        "        # Retrieve context using wrapper function\n",
        "        contexts = get_context_wrapper(searcher, question, k=K, retrieval_method=RETRIEVAL_METHOD)\n",
        "        \n",
        "        if not contexts:\n",
        "            answer = \"I don't know\"\n",
        "        else:\n",
        "            # Create prompt and generate answer\n",
        "            messages = create_message(question, contexts)\n",
        "            outputs = pipeline(\n",
        "                messages,\n",
        "                max_new_tokens=MAX_NEW_TOKENS,\n",
        "                eos_token_id=terminators,\n",
        "                do_sample=DO_SAMPLE,\n",
        "                temperature=TEMPERATURE,\n",
        "                top_p=TOP_P,\n",
        "            )\n",
        "            generated_text = outputs[0][\"generated_text\"][-1].get('content', '')\n",
        "            answer = extract_answer(generated_text)\n",
        "        \n",
        "        predictions_train[qid] = answer\n",
        "        processed_train_ids.add(qid)\n",
        "        \n",
        "        # Save checkpoint periodically (every N questions)\n",
        "        if len(predictions_train) % SAVE_CHECKPOINT_EVERY == 0:\n",
        "            train_checkpoint = {\n",
        "                \"predictions\": predictions_train,\n",
        "                \"processed_ids\": list(processed_train_ids)\n",
        "            }\n",
        "            with open(train_checkpoint_file, 'w') as f:\n",
        "                json.dump(train_checkpoint, f, indent=2)\n",
        "            print(f\"\\nüíæ Checkpoint saved: {len(predictions_train)}/{len(df_train_sample)} predictions\")\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ö†Ô∏è  Error processing question {qid}: {e}\")\n",
        "        predictions_train[qid] = \"I don't know\"  # Fallback answer\n",
        "        processed_train_ids.add(qid)\n",
        "        continue\n",
        "\n",
        "# Final checkpoint save\n",
        "if predictions_train:\n",
        "    train_checkpoint = {\n",
        "        \"predictions\": predictions_train,\n",
        "        \"processed_ids\": list(processed_train_ids)\n",
        "    }\n",
        "    with open(train_checkpoint_file, 'w') as f:\n",
        "        json.dump(train_checkpoint, f, indent=2)\n",
        "    print(f\"\\nüíæ Final checkpoint saved: {len(predictions_train)} predictions\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Formatting Predictions for Evaluation\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Format predictions\n",
        "df_pred_train = pd.DataFrame(list(predictions_train.items()), columns=['id', 'prediction'])\n",
        "df_pred_train = df_pred_train.sort_values('id')\n",
        "df_pred_train[\"prediction\"] = df_pred_train[\"prediction\"].apply(\n",
        "    lambda x: json.dumps([x], ensure_ascii=False)\n",
        ")\n",
        "\n",
        "# Format ground truth (use same sample as predictions)\n",
        "df_gold = df_train_sample.copy()\n",
        "df_gold[\"answers\"] = df_gold[\"answers\"].apply(lambda x: json.dumps(x, ensure_ascii=False))\n",
        "\n",
        "print(f\"‚úÖ Formatted {len(df_pred_train)} predictions\")\n",
        "print(f\"‚úÖ Formatted {len(df_gold)} ground truth answers\")\n",
        "\n",
        "# Evaluate\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Evaluating Performance - Computing F1 Score\")\n",
        "print(\"=\" * 80)\n",
        "f1 = score(df_gold, df_pred_train)\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"üìä EVALUATION RESULTS\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"‚úÖ F1 Score on training set: {f1:.2f}\")\n",
        "print(f\"üìä Baseline F1: 11.62\")\n",
        "print(f\"üìà Improvement: {f1 - 11.62:.2f} points\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "if f1 < 11.62:\n",
        "    print(\"\\n‚ö†Ô∏è  WARNING: Your F1 score is below baseline!\")\n",
        "    print(\"   Consider adjusting parameters before running on test data.\")\n",
        "    print(\"   Suggested parameters to tune:\")\n",
        "    print(\"   - K (number of passages): Try 5, 10, 15, 20\")\n",
        "    print(\"   - QLD_MU: Try 500, 1000, 2000\")\n",
        "    print(\"   - TEMPERATURE: Try 0.3, 0.6, 0.9\")\n",
        "    print(\"   - CONTEXT_LENGTH: Try 400, 800, 1200\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ Your F1 score is above baseline!\")\n",
        "    print(\"   You can proceed to test data processing.\")\n",
        "    print(f\"   Expected test performance: Similar to training F1 ({f1:.2f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Process Test Questions\n",
        "\n",
        "Process all test questions and generate predictions. The system will:\n",
        "1. Load checkpoint if exists (resume from previous run)\n",
        "2. Process all questions with progress bar\n",
        "3. Save checkpoints periodically\n",
        "4. Generate final predictions CSV\n",
        "\n",
        "**Note**: Only run this after you're satisfied with training evaluation results!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load checkpoint if exists\n",
        "predictions, processed_ids = load_checkpoint_wrapper()\n",
        "\n",
        "# Process questions\n",
        "print(\"=\" * 80)\n",
        "print(\"Processing Test Questions\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Retrieval method: {RETRIEVAL_METHOD}, k={K}\")\n",
        "print(f\"Total questions: {len(df_test)}\")\n",
        "print(f\"Already processed: {len(processed_ids)}\")\n",
        "print(f\"Remaining: {len(df_test) - len(processed_ids)}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for index, row in tqdm(df_test.iterrows(), total=len(df_test), desc=\"Processing\"):\n",
        "    qid = row['id']\n",
        "    question = row['question']\n",
        "\n",
        "    # Skip if already processed\n",
        "    if qid in processed_ids:\n",
        "        continue\n",
        "\n",
        "    # Retrieve context using wrapper function\n",
        "    contexts = get_context_wrapper(searcher, question, k=K, retrieval_method=RETRIEVAL_METHOD)\n",
        "\n",
        "    if not contexts:\n",
        "        answer = \"I don't know\"\n",
        "    else:\n",
        "        # Create prompt and generate answer\n",
        "        messages = create_message(question, contexts)\n",
        "        outputs = pipeline(\n",
        "            messages,\n",
        "            max_new_tokens=MAX_NEW_TOKENS,\n",
        "            eos_token_id=terminators,\n",
        "            do_sample=DO_SAMPLE,\n",
        "            temperature=TEMPERATURE,\n",
        "            top_p=TOP_P,\n",
        "        )\n",
        "        generated_text = outputs[0][\"generated_text\"][-1].get('content', '')\n",
        "        answer = extract_answer(generated_text)\n",
        "\n",
        "    predictions[qid] = answer\n",
        "    processed_ids.add(qid)\n",
        "\n",
        "    # Save checkpoint periodically\n",
        "    if len(predictions) % SAVE_CHECKPOINT_EVERY == 0:\n",
        "        save_checkpoint_wrapper(predictions, list(processed_ids))\n",
        "\n",
        "# Final checkpoint save\n",
        "save_checkpoint_wrapper(predictions, list(processed_ids))\n",
        "\n",
        "print(\"\\n‚úÖ Processing complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Format and Save Test Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Format predictions\n",
        "df_prediction = pd.DataFrame(list(predictions.items()), columns=['id', 'prediction'])\n",
        "df_prediction = df_prediction.sort_values('id')\n",
        "\n",
        "# Format predictions as JSON arrays (required format)\n",
        "df_prediction[\"prediction\"] = df_prediction[\"prediction\"].apply(\n",
        "    lambda x: json.dumps([x], ensure_ascii=False)\n",
        ")\n",
        "\n",
        "# Save to CSV\n",
        "df_prediction.to_csv(PREDICTIONS_CSV, index=False)\n",
        "print(f\"‚úÖ Predictions saved to {PREDICTIONS_CSV}\")\n",
        "print(f\"Total predictions: {len(df_prediction)}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
