# RAG System - Code Model Description

## Overview

This document provides a comprehensive technical description of the RAG system codebase, including class structures, function signatures, data flow, and module organization.

---

## Code Structure

### File Organization

```
ex3/
├── rag_system.py          # Main Python script (standalone execution)
├── rag_system.ipynb       # Jupyter notebook version (interactive development)
├── train.csv              # Training data (questions + ground truth answers)
├── test.csv               # Test data (questions only)
├── predictions.csv        # Output predictions (generated by system)
└── checkpoint.json       # Checkpoint file (for resuming interrupted runs)
```

---

## Module Dependencies

### External Libraries

```python
import json              # JSON parsing and serialization
import os                # Environment variables and file operations
import re                # Regular expressions for text processing
import string            # String constants and utilities
from collections import Counter  # Token counting for F1 score
from pathlib import Path         # Path manipulation

import pandas as pd              # Data manipulation (CSV reading/writing)
import torch                     # PyTorch (for model loading)
import transformers              # HuggingFace transformers library
from huggingface_hub import login  # HuggingFace authentication
from pyserini.search import SimpleSearcher  # Information retrieval
from tqdm import tqdm            # Progress bars
```

### Key Dependencies

- **pandas**: Data loading and manipulation
- **transformers**: LLM pipeline and model loading
- **pyserini**: Wikipedia index search and retrieval
- **torch**: Deep learning framework (for model execution)

---

## Class Definitions

### Config Class

**Location**: `rag_system.py`, lines 25-57

**Purpose**: Centralized configuration management for all system parameters.

**Structure**:
```python
class Config:
    """Configuration parameters for the RAG system."""
    
    # Data paths
    TRAIN_CSV = "./train.csv"
    TEST_CSV = "./test.csv"
    PREDICTIONS_CSV = "./predictions.csv"
    CHECKPOINT_FILE = "./checkpoint.json"
    
    # HuggingFace token
    HF_TOKEN = os.getenv("KAGGLE_API_TOKEN", "default_token")
    
    # Retrieval parameters
    K = 30                    # Number of passages to retrieve
    RETRIEVAL_METHOD = "qld"  # "qld" or "bm25"
    QLD_MU = 1000             # Dirichlet smoothing parameter
    BM25_K1 = 0.9             # BM25 k1 parameter
    BM25_B = 0.4              # BM25 b parameter
    CONTEXT_LENGTH = 800       # Max characters per passage (0 = no limit)
    
    # LLM parameters
    MODEL_ID = "meta-llama/Llama-3.2-1B-Instruct"
    MAX_NEW_TOKENS = 64
    TEMPERATURE = 0.6
    TOP_P = 0.9
    DO_SAMPLE = True
    
    # Processing
    BATCH_SIZE = 1
    SAVE_CHECKPOINT_EVERY = 50
    RESUME_FROM_CHECKPOINT = True
```

**Usage**: Instantiate to override default parameters:
```python
config = Config()
config.K = 20  # Override default
config.TEMPERATURE = 0.0  # Override default
```

---

## Function Definitions

### Evaluation Module

#### `normalize_answer(s: str) -> str`

**Location**: `rag_system.py`, lines 63-77

**Purpose**: Normalize text for F1 score computation by removing punctuation, articles, and normalizing whitespace.

**Parameters**:
- `s` (str): Input text to normalize

**Returns**:
- `str`: Normalized text (lowercase, no punctuation, no articles, single spaces)

**Process**:
1. Remove articles (a, an, the)
2. Remove punctuation
3. Convert to lowercase
4. Normalize whitespace

**Example**:
```python
normalize_answer("The capital of France is Paris.") 
# Returns: "capital of france is paris"
```

---

#### `f1_score(prediction: str, ground_truth: str) -> float`

**Location**: `rag_system.py`, lines 80-95

**Purpose**: Compute token-level F1 score between prediction and ground truth.

**Parameters**:
- `prediction` (str): Predicted answer
- `ground_truth` (str): Correct answer

**Returns**:
- `float`: F1 score (0.0 to 1.0)

**Formula**:
```
precision = common_tokens / prediction_tokens
recall = common_tokens / ground_truth_tokens
f1 = 2 * (precision * recall) / (precision + recall)
```

**Example**:
```python
f1_score("Paris", "Paris, France")  # Returns: 0.67 (1 common token / 1.5 avg tokens)
```

---

#### `metric_max_over_ground_truths(metric_fn, prediction, ground_truths) -> float`

**Location**: `rag_system.py`, lines 98-99

**Purpose**: Compute maximum metric score over multiple ground truth answers.

**Parameters**:
- `metric_fn`: Function to compute metric (e.g., `f1_score`)
- `prediction` (str): Predicted answer
- `ground_truths` (list): List of possible correct answers

**Returns**:
- `float`: Maximum metric score

**Use Case**: When multiple answers are acceptable, take the best match.

---

#### `score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str = 'id') -> float`

**Location**: `rag_system.py`, lines 102-135

**Purpose**: Compute average F1 score over all questions (for leaderboard ranking).

**Parameters**:
- `solution` (pd.DataFrame): Ground truth data with 'id' and 'answers' columns
- `submission` (pd.DataFrame): Predictions with 'id' and 'prediction' columns
- `row_id_column_name` (str): Column name for question IDs (default: 'id')

**Returns**:
- `float`: Average F1 score (0.0 to 100.0)

**Process**:
1. Align dataframes by ID
2. For each question:
   - Parse ground truth answers (JSON list)
   - Compute F1 score with prediction
   - Take maximum over all ground truth answers
3. Return average F1 score × 100

**Example**:
```python
score(gold_df, pred_df)  # Returns: 45.23 (average F1 score)
```

---

### Retrieval Module

#### `get_context_qld(searcher, query: str, k: int, mu: float = 1000) -> list`

**Location**: `rag_system.py`, lines 142-146

**Purpose**: Retrieve context passages using Query Likelihood Dirichlet (QLD) method.

**Parameters**:
- `searcher` (SimpleSearcher): Pyserini searcher instance
- `query` (str): Question string
- `k` (int): Number of passages to retrieve
- `mu` (float): Dirichlet smoothing parameter (default: 1000)

**Returns**:
- `list`: List of hit objects (ranked by relevance)

**Process**:
1. Set searcher to QLD mode with specified mu
2. Search Wikipedia index
3. Return top-k hits

---

#### `get_context_bm25(searcher, query: str, k: int, k1: float = 0.9, b: float = 0.4) -> list`

**Location**: `rag_system.py`, lines 149-153

**Purpose**: Retrieve context passages using BM25 method.

**Parameters**:
- `searcher` (SimpleSearcher): Pyserini searcher instance
- `query` (str): Question string
- `k` (int): Number of passages to retrieve
- `k1` (float): BM25 term frequency saturation (default: 0.9)
- `b` (float): BM25 length normalization (default: 0.4)

**Returns**:
- `list`: List of hit objects (ranked by relevance)

**Process**:
1. Set searcher to BM25 mode with specified parameters
2. Search Wikipedia index
3. Return top-k hits

---

#### `get_context(searcher, query: str, k: int = 10, retrieval_method: str = "qld", config: Config = None) -> list[str]`

**Location**: `rag_system.py`, lines 160-209

**Purpose**: Main retrieval function that retrieves and extracts passage text.

**Parameters**:
- `searcher` (SimpleSearcher): Pyserini searcher instance
- `query` (str): Question string
- `k` (int): Number of passages to retrieve (default: 10)
- `retrieval_method` (str): "qld" (primary) or "bm25" (optional)
- `config` (Config): Configuration object (optional, uses default if None)

**Returns**:
- `list[str]`: List of context passage strings

**Process**:
1. Retrieve hits using specified method (QLD or BM25)
2. For each hit:
   - Extract document by docid
   - Parse JSON content
   - Extract 'contents' field
   - Clean (replace newlines with spaces)
   - Truncate if CONTEXT_LENGTH > 0
3. Return list of context strings

**Error Handling**: Skips documents that cannot be retrieved (prints warning).

**Example**:
```python
contexts = get_context(searcher, "What is the capital of France?", k=30, retrieval_method="qld")
# Returns: ["Paris is the capital...", "France's capital city...", ...]
```

---

### Prompt Engineering Module

#### `create_message(query: str, contexts: list[str]) -> list[dict]`

**Location**: `rag_system.py`, lines 216-249

**Purpose**: Create prompt messages for LLM in ChatML format.

**Parameters**:
- `query` (str): Question string
- `contexts` (list[str]): List of retrieved context passages

**Returns**:
- `list[dict]`: List of message dictionaries with 'role' and 'content' keys

**Message Structure**:
```python
[
    {
        "role": "system",
        "content": system_prompt  # Instructions and rules
    },
    {
        "role": "user",
        "content": user_prompt   # Context + Question + "Answer:"
    }
]
```

**System Prompt Content**:
- Instructions to use only provided passages
- Rules for concise answers (1-5 words)
- Instructions to avoid explanations
- Fallback to "I don't know" if answer not found

**User Prompt Format**:
```
Based on the following passages, provide a concise answer to the question.

Passages:
Passage 1: [context 1]
Passage 2: [context 2]
...

Question: [query]

Answer:
```

**Example**:
```python
messages = create_message("What is the capital of France?", ["Paris is the capital...", ...])
# Returns: [{"role": "system", "content": "..."}, {"role": "user", "content": "..."}]
```

---

#### `extract_answer(text: str) -> str`

**Location**: `rag_system.py`, lines 252-277

**Purpose**: Extract clean answer from LLM output by removing explanations and meta-language.

**Parameters**:
- `text` (str): Raw LLM output text

**Returns**:
- `str`: Extracted answer (cleaned)

**Process**:
1. Remove common prefixes ("The answer is", "Answer:", etc.)
2. Split into sentences
3. Take first sentence if reasonable length (≤10 words)
4. Fallback: Return first 50 characters

**Example**:
```python
extract_answer("The answer is Paris, which is the capital of France.")
# Returns: "Paris"
```

---

### LLM Module

#### `load_llm_pipeline(config: Config = None) -> transformers.Pipeline`

**Location**: `rag_system.py`, lines 284-308

**Purpose**: Load Llama-3.2-1B-Instruct model and create text-generation pipeline.

**Parameters**:
- `config` (Config): Configuration object (optional, uses default if None)

**Returns**:
- `transformers.Pipeline`: Text generation pipeline

**Process**:
1. Authenticate with HuggingFace using token from config
2. Load model using transformers pipeline:
   - Model: `meta-llama/Llama-3.2-1B-Instruct`
   - Task: "text-generation"
   - Dtype: bfloat16 (for memory efficiency)
   - Device: auto (CPU/GPU)
3. Return pipeline

**Error Handling**: Raises exception if model loading fails.

**Example**:
```python
pipeline = load_llm_pipeline(config)
# Returns: Pipeline object ready for text generation
```

---

#### `llm_answer(pipeline, searcher, query: str, config: Config = None) -> str`

**Location**: `rag_system.py`, lines 311-363

**Purpose**: Complete RAG pipeline: retrieve context, generate prompt, call LLM, extract answer.

**Parameters**:
- `pipeline` (transformers.Pipeline): Text generation pipeline
- `searcher` (SimpleSearcher): Pyserini searcher instance
- `query` (str): Question string
- `config` (Config): Configuration object (optional)

**Returns**:
- `str`: Final answer string

**Process**:
1. **Retrieve context**: Call `get_context()` with query and config
2. **Check empty**: Return "I don't know" if no contexts
3. **Create prompt**: Call `create_message()` with query and contexts
4. **Set terminators**: EOS token + `<|eot_id|>` token
5. **Generate answer**: Call pipeline with messages and parameters:
   - `max_new_tokens`: From config
   - `eos_token_id`: Termination tokens
   - `do_sample`: From config
   - `temperature`: From config
   - `top_p`: From config
6. **Extract answer**: Call `extract_answer()` on generated text
7. **Return**: Final answer

**Error Handling**: Returns "I don't know" if any error occurs.

**Example**:
```python
answer = llm_answer(pipeline, searcher, "What is the capital of France?", config)
# Returns: "Paris"
```

---

### Checkpointing Module

#### `save_checkpoint(predictions: dict, processed_ids: set) -> None`

**Location**: `rag_system.py`, lines 370-378

**Purpose**: Save checkpoint to resume processing later.

**Parameters**:
- `predictions` (dict): Dictionary mapping question ID to answer
- `processed_ids` (set): Set of processed question IDs

**Process**:
1. Create checkpoint dictionary:
   ```python
   {
       "predictions": predictions,
       "processed_ids": list(processed_ids)
   }
   ```
2. Write to JSON file (Config.CHECKPOINT_FILE)
3. Print confirmation message

**File Format**: JSON with indentation for readability.

---

#### `load_checkpoint() -> tuple[dict, set]`

**Location**: `rag_system.py`, lines 381-391

**Purpose**: Load checkpoint if exists to resume processing.

**Returns**:
- `tuple[dict, set]`: (predictions dictionary, processed_ids set)

**Process**:
1. Check if checkpoint file exists and RESUME_FROM_CHECKPOINT is True
2. Load JSON file
3. Extract predictions and processed_ids
4. Convert processed_ids list to set
5. Return tuple

**Error Handling**: Returns empty dict and set if file doesn't exist or error occurs.

**Example**:
```python
predictions, processed_ids = load_checkpoint()
# Returns: ({1: "Paris", 2: "London"}, {1, 2})
```

---

### Main Processing Module

#### `process_test_questions(config: Config = None) -> pd.DataFrame`

**Location**: `rag_system.py`, lines 398-464

**Purpose**: Process all test questions and generate predictions CSV file.

**Parameters**:
- `config` (Config): Configuration object (optional)

**Returns**:
- `pd.DataFrame`: DataFrame with predictions

**Process**:
1. **Load test data**: Read CSV file (config.TEST_CSV)
2. **Initialize searcher**: Load Wikipedia KILT index
3. **Load LLM**: Call `load_llm_pipeline()`
4. **Load checkpoint**: Call `load_checkpoint()` (if resuming)
5. **Process questions**:
   - For each question in test set:
     - Skip if already processed (from checkpoint)
     - Call `llm_answer()` to generate answer
     - Store prediction
     - Save checkpoint periodically (every N questions)
6. **Format predictions**:
   - Convert to DataFrame
   - Sort by ID
   - Format prediction as JSON list: `["answer"]`
7. **Save to CSV**: Write to config.PREDICTIONS_CSV
8. **Return**: Predictions DataFrame

**Output Format**:
```csv
id,prediction
1,"[\"Paris\"]"
2,"[\"London\"]"
```

---

#### `evaluate_on_train(config: Config = None) -> tuple[float, pd.DataFrame]`

**Location**: `rag_system.py`, lines 467-518

**Purpose**: Evaluate system on training set to compute F1 score.

**Parameters**:
- `config` (Config): Configuration object (optional)

**Returns**:
- `tuple[float, pd.DataFrame]`: (F1 score, predictions DataFrame)

**Process**:
1. **Load training data**: Read CSV with answers (JSON format)
2. **Initialize searcher**: Load Wikipedia KILT index
3. **Load LLM**: Call `load_llm_pipeline()`
4. **Process questions**: For each question, generate answer
5. **Format predictions**: Convert to DataFrame with JSON format
6. **Format ground truth**: Convert answers to JSON strings
7. **Evaluate**: Call `score()` function to compute F1
8. **Print results**: F1 score, baseline comparison
9. **Return**: F1 score and predictions DataFrame

**Example**:
```python
f1, df_pred = evaluate_on_train(config)
# Prints: "F1 Score on training set: 45.23"
# Returns: (45.23, predictions_df)
```

---

## Data Flow

### Complete Processing Flow

```
1. Question Input
   └─> CSV file (test.csv or train.csv)
       └─> DataFrame with 'id' and 'question' columns

2. Retrieval Phase
   └─> get_context(searcher, query, k, method, config)
       └─> get_context_qld() or get_context_bm25()
           └─> searcher.search(query, k)
               └─> List of hits (ranked passages)
                   └─> Extract document content
                       └─> List of context strings

3. Prompt Creation
   └─> create_message(query, contexts)
       └─> Format system prompt (instructions)
       └─> Format user prompt (contexts + question)
           └─> List of messages (ChatML format)

4. LLM Generation
   └─> pipeline(messages, max_new_tokens, ...)
       └─> Llama-3.2-1B-Instruct model
           └─> Generated text (may be verbose)

5. Answer Extraction
   └─> extract_answer(generated_text)
       └─> Remove prefixes
       └─> Extract first sentence
       └─> Clean and truncate
           └─> Final answer string

6. Output
   └─> Store in predictions dictionary
       └─> Save to CSV file (predictions.csv)
```

### Data Structures

#### Input Data Format

**Training CSV** (`train.csv`):
```csv
id,question,answers
1,"What is the capital of France?","[\"Paris\", \"Paris, France\"]"
2,"Who wrote Romeo and Juliet?","[\"William Shakespeare\", \"Shakespeare\"]"
```

**Test CSV** (`test.csv`):
```csv
id,question
1,"What is the capital of France?"
2,"Who wrote Romeo and Juliet?"
```

#### Output Data Format

**Predictions CSV** (`predictions.csv`):
```csv
id,prediction
1,"[\"Paris\"]"
2,"[\"William Shakespeare\"]"
```

**Checkpoint JSON** (`checkpoint.json`):
```json
{
  "predictions": {
    "1": "Paris",
    "2": "William Shakespeare"
  },
  "processed_ids": [1, 2]
}
```

---

## Module Interactions

### Dependency Graph

```
rag_system.py
├── Config (class)
│   └─> Used by: All functions
│
├── Evaluation Module
│   ├── normalize_answer()
│   ├── f1_score()
│   ├── metric_max_over_ground_truths()
│   └── score()
│       └─> Uses: normalize_answer(), f1_score(), metric_max_over_ground_truths()
│
├── Retrieval Module
│   ├── get_context_qld()
│   ├── get_context_bm25()
│   └── get_context()
│       └─> Uses: get_context_qld() or get_context_bm25()
│
├── Prompt Module
│   ├── create_message()
│   └── extract_answer()
│
├── LLM Module
│   ├── load_llm_pipeline()
│   └── llm_answer()
│       └─> Uses: get_context(), create_message(), extract_answer()
│
├── Checkpointing Module
│   ├── save_checkpoint()
│   └── load_checkpoint()
│
└── Main Processing Module
    ├── process_test_questions()
    │   └─> Uses: load_llm_pipeline(), load_checkpoint(), llm_answer(), save_checkpoint()
    └── evaluate_on_train()
        └─> Uses: load_llm_pipeline(), llm_answer(), score()
```

---

## Entry Points

### Command-Line Interface

**Location**: `rag_system.py`, lines 525-550

**Usage**:
```bash
# Process test questions (default)
python rag_system.py

# Evaluate on training set
python rag_system.py --mode train

# Both test and train
python rag_system.py --mode both

# Custom parameters
python rag_system.py --k 20 --method qld --qld-mu 1500
```

**Arguments**:
- `--mode`: "test", "train", or "both" (default: "test")
- `--k`: Number of passages to retrieve (default: Config.K)
- `--method`: "qld" or "bm25" (default: Config.RETRIEVAL_METHOD)
- `--qld-mu`: QLD mu parameter (default: Config.QLD_MU)

### Jupyter Notebook

**Location**: `rag_system.ipynb`

**Usage**:
1. Run cells sequentially
2. Configuration in Cell 7
3. Processing in later cells

**Key Cells**:
- Cell 0: HuggingFace authentication
- Cell 1-2: Environment setup (Java, dependencies)
- Cell 3-4: Initialize searcher and display stats
- Cell 5-6: Load data
- Cell 7: Configuration parameters
- Cell 8+: Processing and evaluation

---

## Error Handling

### Exception Handling Strategy

**Retrieval Errors**:
```python
try:
    doc = searcher.doc(hit.docid)
    # ... extract content
except Exception as e:
    print(f"Warning: Could not retrieve document {hit.docid}: {e}")
    continue  # Skip this passage, continue with others
```

**LLM Errors**:
```python
try:
    answer = llm_answer(pipeline, searcher, question, config)
except Exception as e:
    print(f"Error generating answer for query '{query}': {e}")
    return "I don't know"  # Fallback answer
```

**Checkpoint Errors**:
```python
try:
    checkpoint = json.load(f)
    # ... process checkpoint
except Exception as e:
    print(f"Error loading checkpoint: {e}")
    return {}, set()  # Return empty checkpoint
```

**Model Loading Errors**:
```python
try:
    pipeline = transformers.pipeline(...)
except Exception as e:
    print(f"Error loading LLM: {e}")
    raise  # Re-raise (critical error)
```

---

## Configuration Management

### Parameter Hierarchy

1. **Default Values**: Defined in `Config` class
2. **Command-Line Arguments**: Override defaults via `argparse`
3. **Runtime Override**: Modify `config` object before calling functions

**Example**:
```python
# Use defaults
config = Config()

# Override via command-line
python rag_system.py --k 20 --qld-mu 1500

# Override programmatically
config = Config()
config.K = 20
config.QLD_MU = 1500
process_test_questions(config)
```

---

## Performance Characteristics

### Time Complexity

- **Retrieval**: O(k × log N) where k=passages, N=index size
- **LLM Generation**: O(max_tokens × model_size)
- **Answer Extraction**: O(n) where n=text length
- **Overall**: ~1-2 seconds per question (depends on hardware)

### Memory Usage

- **Model Loading**: ~2-4 GB (Llama-3.2-1B-Instruct in bfloat16)
- **Index Loading**: ~10-20 GB (Wikipedia KILT index, loaded by Pyserini)
- **Processing**: Minimal additional memory per question

### Scalability

- **Sequential Processing**: One question at a time (BATCH_SIZE=1)
- **Checkpointing**: Saves progress every N questions
- **Resume Capability**: Can resume from checkpoint if interrupted

---

## Code Quality Features

### Documentation

- **Docstrings**: All functions have docstrings with purpose, parameters, returns
- **Comments**: Inline comments explain complex logic
- **Type Hints**: Some functions include type hints (e.g., `-> str`)

### Modularity

- **Separation of Concerns**: Each module handles one aspect (retrieval, prompt, LLM, etc.)
- **Reusability**: Functions can be called independently
- **Configurable**: All parameters centralized in Config class

### Maintainability

- **Clear Naming**: Function and variable names are descriptive
- **Error Handling**: Graceful error handling with fallbacks
- **Checkpointing**: Prevents data loss on interruption

---

## Testing and Validation

### Evaluation Functions

- **F1 Score**: Token-level F1 for answer matching
- **Normalization**: Consistent text normalization for comparison
- **Multiple Ground Truths**: Handles multiple acceptable answers

### Validation Points

1. **Input Validation**: CSV format checking
2. **Retrieval Validation**: Empty context handling
3. **Output Validation**: Answer format validation
4. **Checkpoint Validation**: Resume capability verification

---

## Summary

This RAG system codebase is organized into clear modules:

1. **Configuration**: `Config` class centralizes all parameters
2. **Evaluation**: F1 score computation and normalization
3. **Retrieval**: QLD and BM25 retrieval methods
4. **Prompt Engineering**: Message creation and answer extraction
5. **LLM Integration**: Model loading and text generation
6. **Checkpointing**: Progress saving and resuming
7. **Main Processing**: Test and training set processing

The code follows a clear data flow: Question → Retrieval → Prompt → Generation → Extraction → Answer, with error handling and checkpointing for robustness.

---

*This model description documents the code structure as of the current implementation. All functions, classes, and data structures are described with their purposes, parameters, and usage examples.*


