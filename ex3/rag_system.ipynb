{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hugging_face_1bLLamaInstruct = \"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\"\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(hugging_face_1bLLamaInstruct)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y openjdk-21-jdk\n",
        "!update-alternatives --install /usr/bin/java java /usr/lib/jvm/java-21-openjdk-amd64/bin/java 1\n",
        "!update-alternatives --install /usr/bin/javac javac /usr/lib/jvm/java-21-openjdk-amd64/bin/javac 1\n",
        "!update-alternatives --set java /usr/lib/jvm/java-21-openjdk-amd64/bin/java\n",
        "!update-alternatives --set javac /usr/lib/jvm/java-21-openjdk-amd64/bin/javac\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install faiss-cpu --no-cache\n",
        "!pip install pyserini==0.36.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize searcher with Wikipedia KILT index\n",
        "from pyserini.search import SimpleSearcher\n",
        "searcher = SimpleSearcher.from_prebuilt_index('wikipedia-kilt-doc')\n",
        "\n",
        "# Display index statistics\n",
        "from pyserini.index.lucene import IndexReader\n",
        "index_reader = IndexReader.from_prebuilt_index('wikipedia-kilt-doc')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(index_reader.stats())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "df_train = pd.read_csv(\"./train.csv\", converters={\"answers\": json.loads})\n",
        "df_test = pd.read_csv(\"./test.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train.head()\n",
        "df_test.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Configuration - Edit these values as needed\n",
        "# ============================================================================\n",
        "\n",
        "# Data paths (adjust if your files are in a different location)\n",
        "TRAIN_CSV = \"./train.csv\"\n",
        "TEST_CSV = \"./test.csv\"\n",
        "PREDICTIONS_CSV = \"./predictions.csv\"\n",
        "CHECKPOINT_FILE = \"./checkpoint.json\"\n",
        "\n",
        "# Retrieval parameters\n",
        "K = 10  # Number of passages to retrieve\n",
        "RETRIEVAL_METHOD = \"qld\"  # \"qld\" (primary, from course) or \"bm25\" (optional)\n",
        "QLD_MU = 1000  # Dirichlet smoothing parameter for QLD\n",
        "BM25_K1 = 0.9  # BM25 k1 parameter (if using BM25)\n",
        "BM25_B = 0.4  # BM25 b parameter (if using BM25)\n",
        "CONTEXT_LENGTH = 800  # Max characters per passage (0 = no limit)\n",
        "\n",
        "# LLM parameters\n",
        "MAX_NEW_TOKENS = 256\n",
        "TEMPERATURE = 0.6\n",
        "TOP_P = 0.9\n",
        "DO_SAMPLE = True\n",
        "\n",
        "# Processing\n",
        "SAVE_CHECKPOINT_EVERY = 50  # Save checkpoint every N questions\n",
        "RESUME_FROM_CHECKPOINT = True  # Resume if checkpoint exists\n",
        "\n",
        "print(\"âœ… Configuration loaded\")\n",
        "print(f\"Retrieval method: {RETRIEVAL_METHOD}, k={K}, QLD_mu={QLD_MU}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import functions from rag_system.py\n",
        "# Make sure rag_system.py is uploaded to your Colab environment\n",
        "from rag_system import (\n",
        "    get_context_qld,\n",
        "    get_context_bm25,\n",
        "    create_message,\n",
        "    extract_answer,\n",
        "    normalize_answer,\n",
        "    f1_score,\n",
        "    metric_max_over_ground_truths,\n",
        "    score,\n",
        "    load_llm_pipeline\n",
        ")\n",
        "from pathlib import Path\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Create a simple config class for compatibility with rag_system.py functions\n",
        "class SimpleConfig:\n",
        "    \"\"\"Simple config class that uses notebook variables\"\"\"\n",
        "    def __init__(self):\n",
        "        self.QLD_MU = QLD_MU\n",
        "        self.BM25_K1 = BM25_K1\n",
        "        self.BM25_B = BM25_B\n",
        "        self.CONTEXT_LENGTH = CONTEXT_LENGTH\n",
        "        self.CHECKPOINT_FILE = Path(CHECKPOINT_FILE)\n",
        "        self.RESUME_FROM_CHECKPOINT = RESUME_FROM_CHECKPOINT\n",
        "\n",
        "# Wrapper function for get_context that uses notebook config\n",
        "def get_context_wrapper(searcher, query, k, retrieval_method):\n",
        "    \"\"\"Wrapper for get_context that uses notebook config variables\"\"\"\n",
        "    if retrieval_method == \"qld\":\n",
        "        hits = get_context_qld(searcher, query, k, mu=QLD_MU)\n",
        "    elif retrieval_method == \"bm25\":\n",
        "        hits = get_context_bm25(searcher, query, k, k1=BM25_K1, b=BM25_B)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown retrieval method: {retrieval_method}\")\n",
        "    \n",
        "    # Extract passage text\n",
        "    contexts = []\n",
        "    for hit in hits:\n",
        "        try:\n",
        "            doc = searcher.doc(hit.docid)\n",
        "            raw_json = doc.raw()\n",
        "            data = json.loads(raw_json)\n",
        "            contents = data['contents']\n",
        "            \n",
        "            # Clean and truncate if needed\n",
        "            content = contents.replace('\\n', ' ')\n",
        "            if CONTEXT_LENGTH > 0 and len(content) > CONTEXT_LENGTH:\n",
        "                content = content[:CONTEXT_LENGTH] + \"...\"\n",
        "            \n",
        "            contexts.append(content)\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not retrieve document {hit.docid}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    return contexts\n",
        "\n",
        "# Wrapper functions for checkpointing that use notebook config\n",
        "def save_checkpoint_wrapper(predictions, processed_ids):\n",
        "    \"\"\"Save checkpoint using notebook config\"\"\"\n",
        "    checkpoint = {\n",
        "        \"predictions\": predictions,\n",
        "        \"processed_ids\": processed_ids\n",
        "    }\n",
        "    with open(CHECKPOINT_FILE, 'w') as f:\n",
        "        json.dump(checkpoint, f, indent=2)\n",
        "    print(f\"Checkpoint saved: {len(predictions)} predictions\")\n",
        "\n",
        "def load_checkpoint_wrapper():\n",
        "    \"\"\"Load checkpoint using notebook config\"\"\"\n",
        "    checkpoint_path = Path(CHECKPOINT_FILE)\n",
        "    if checkpoint_path.exists() and RESUME_FROM_CHECKPOINT:\n",
        "        try:\n",
        "            with open(CHECKPOINT_FILE, 'r') as f:\n",
        "                checkpoint = json.load(f)\n",
        "            print(f\"Checkpoint loaded: {len(checkpoint['predictions'])} predictions\")\n",
        "            return checkpoint[\"predictions\"], set(checkpoint[\"processed_ids\"])\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading checkpoint: {e}\")\n",
        "    return {}, set()\n",
        "\n",
        "print(\"âœ… RAG system functions imported\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup Retrieval and LLM\n",
        "\n",
        "Configure the searcher and get terminators for LLM generation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set retrieval method\n",
        "if RETRIEVAL_METHOD == \"qld\":\n",
        "    searcher.set_qld(mu=QLD_MU)\n",
        "elif RETRIEVAL_METHOD == \"bm25\":\n",
        "    searcher.set_bm25(k1=BM25_K1, b=BM25_B)\n",
        "\n",
        "# Load LLM pipeline\n",
        "print(\"Loading LLM pipeline...\")\n",
        "pipeline = load_llm_pipeline()\n",
        "print(\"âœ… LLM pipeline loaded\")\n",
        "\n",
        "# Get terminators for LLM generation\n",
        "terminators = [\n",
        "    pipeline.tokenizer.eos_token_id,\n",
        "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n",
        "\n",
        "print(f\"âœ… Searcher configured with method: {RETRIEVAL_METHOD}\")\n",
        "print(\"âœ… LLM terminators ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate on Training Set (Run First!)\n",
        "\n",
        "Evaluate the system on the training set to compute F1 score and compare with baseline.\n",
        "**Only proceed to test data after you're satisfied with training results.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process training questions for evaluation\n",
        "predictions_train = {}\n",
        "print(\"=\" * 80)\n",
        "print(\"Evaluating on Training Set\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Retrieval method: {RETRIEVAL_METHOD}, k={K}\")\n",
        "print(f\"Total training questions: {len(df_train)}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for index, row in tqdm(df_train.iterrows(), total=len(df_train), desc=\"Processing training questions\"):\n",
        "    question = row['question']\n",
        "    qid = row['id']\n",
        "    \n",
        "    # Retrieve context using wrapper function\n",
        "    contexts = get_context_wrapper(searcher, question, k=K, retrieval_method=RETRIEVAL_METHOD)\n",
        "    \n",
        "    if not contexts:\n",
        "        answer = \"I don't know\"\n",
        "    else:\n",
        "        # Create prompt and generate answer\n",
        "        messages = create_message(question, contexts)\n",
        "        outputs = pipeline(\n",
        "            messages,\n",
        "            max_new_tokens=MAX_NEW_TOKENS,\n",
        "            eos_token_id=terminators,\n",
        "            do_sample=DO_SAMPLE,\n",
        "            temperature=TEMPERATURE,\n",
        "            top_p=TOP_P,\n",
        "        )\n",
        "        generated_text = outputs[0][\"generated_text\"][-1].get('content', '')\n",
        "        answer = extract_answer(generated_text)\n",
        "    \n",
        "    predictions_train[qid] = answer\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Formatting Predictions for Evaluation\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Format predictions\n",
        "df_pred_train = pd.DataFrame(list(predictions_train.items()), columns=['id', 'prediction'])\n",
        "df_pred_train = df_pred_train.sort_values('id')\n",
        "df_pred_train[\"prediction\"] = df_pred_train[\"prediction\"].apply(\n",
        "    lambda x: json.dumps([x], ensure_ascii=False)\n",
        ")\n",
        "\n",
        "# Format ground truth\n",
        "df_gold = df_train.copy()\n",
        "df_gold[\"answers\"] = df_gold[\"answers\"].apply(lambda x: json.dumps(x, ensure_ascii=False))\n",
        "\n",
        "print(f\"âœ… Formatted {len(df_pred_train)} predictions\")\n",
        "print(f\"âœ… Formatted {len(df_gold)} ground truth answers\")\n",
        "\n",
        "# Evaluate\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Evaluating Performance - Computing F1 Score\")\n",
        "print(\"=\" * 80)\n",
        "f1 = score(df_gold, df_pred_train)\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"ðŸ“Š EVALUATION RESULTS\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"âœ… F1 Score on training set: {f1:.2f}\")\n",
        "print(f\"ðŸ“Š Baseline F1: 11.62\")\n",
        "print(f\"ðŸ“ˆ Improvement: {f1 - 11.62:.2f} points\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "if f1 < 11.62:\n",
        "    print(\"\\nâš ï¸  WARNING: Your F1 score is below baseline!\")\n",
        "    print(\"   Consider adjusting parameters before running on test data.\")\n",
        "    print(\"   Suggested parameters to tune:\")\n",
        "    print(\"   - K (number of passages): Try 5, 10, 15, 20\")\n",
        "    print(\"   - QLD_MU: Try 500, 1000, 2000\")\n",
        "    print(\"   - TEMPERATURE: Try 0.3, 0.6, 0.9\")\n",
        "    print(\"   - CONTEXT_LENGTH: Try 400, 800, 1200\")\n",
        "else:\n",
        "    print(\"\\nâœ… Your F1 score is above baseline!\")\n",
        "    print(\"   You can proceed to test data processing.\")\n",
        "    print(f\"   Expected test performance: Similar to training F1 ({f1:.2f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Process Test Questions\n",
        "\n",
        "Process all test questions and generate predictions. The system will:\n",
        "1. Load checkpoint if exists (resume from previous run)\n",
        "2. Process all questions with progress bar\n",
        "3. Save checkpoints periodically\n",
        "4. Generate final predictions CSV\n",
        "\n",
        "**Note**: Only run this after you're satisfied with training evaluation results!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load checkpoint if exists\n",
        "predictions, processed_ids = load_checkpoint_wrapper()\n",
        "\n",
        "# Process questions\n",
        "print(\"=\" * 80)\n",
        "print(\"Processing Test Questions\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Retrieval method: {RETRIEVAL_METHOD}, k={K}\")\n",
        "print(f\"Total questions: {len(df_test)}\")\n",
        "print(f\"Already processed: {len(processed_ids)}\")\n",
        "print(f\"Remaining: {len(df_test) - len(processed_ids)}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for index, row in tqdm(df_test.iterrows(), total=len(df_test), desc=\"Processing\"):\n",
        "    qid = row['id']\n",
        "    question = row['question']\n",
        "    \n",
        "    # Skip if already processed\n",
        "    if qid in processed_ids:\n",
        "        continue\n",
        "    \n",
        "    # Retrieve context using wrapper function\n",
        "    contexts = get_context_wrapper(searcher, question, k=K, retrieval_method=RETRIEVAL_METHOD)\n",
        "    \n",
        "    if not contexts:\n",
        "        answer = \"I don't know\"\n",
        "    else:\n",
        "        # Create prompt and generate answer\n",
        "        messages = create_message(question, contexts)\n",
        "        outputs = pipeline(\n",
        "            messages,\n",
        "            max_new_tokens=MAX_NEW_TOKENS,\n",
        "            eos_token_id=terminators,\n",
        "            do_sample=DO_SAMPLE,\n",
        "            temperature=TEMPERATURE,\n",
        "            top_p=TOP_P,\n",
        "        )\n",
        "        generated_text = outputs[0][\"generated_text\"][-1].get('content', '')\n",
        "        answer = extract_answer(generated_text)\n",
        "    \n",
        "    predictions[qid] = answer\n",
        "    processed_ids.add(qid)\n",
        "    \n",
        "    # Save checkpoint periodically\n",
        "    if len(predictions) % SAVE_CHECKPOINT_EVERY == 0:\n",
        "        save_checkpoint_wrapper(predictions, list(processed_ids))\n",
        "\n",
        "# Final checkpoint save\n",
        "save_checkpoint_wrapper(predictions, list(processed_ids))\n",
        "\n",
        "print(\"\\nâœ… Processing complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Format and Save Test Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Format predictions\n",
        "df_prediction = pd.DataFrame(list(predictions.items()), columns=['id', 'prediction'])\n",
        "df_prediction = df_prediction.sort_values('id')\n",
        "\n",
        "# Format predictions as JSON arrays (required format)\n",
        "df_prediction[\"prediction\"] = df_prediction[\"prediction\"].apply(\n",
        "    lambda x: json.dumps([x], ensure_ascii=False)\n",
        ")\n",
        "\n",
        "# Save to CSV\n",
        "df_prediction.to_csv(PREDICTIONS_CSV, index=False)\n",
        "print(f\"âœ… Predictions saved to {PREDICTIONS_CSV}\")\n",
        "print(f\"Total predictions: {len(df_prediction)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Process Test Questions\n",
        "\n",
        "Process all test questions and generate predictions. **Fixed**: Processes all 2032 questions instead of just 5.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process all test questions\n",
        "predictions_LLM = {}\n",
        "\n",
        "print(f\"Processing {len(df_test)} test questions...\")\n",
        "print(f\"Retrieval method: {RETRIEVAL_METHOD}, k={K}\")\n",
        "\n",
        "for index, row in tqdm(df_test.iterrows(), total=len(df_test), desc=\"Processing questions\"):\n",
        "    question = row['question']\n",
        "    qid = row['id']\n",
        "    \n",
        "    answer = llm_answer(question)\n",
        "    predictions_LLM[qid] = answer\n",
        "\n",
        "print(f\"\\nCompleted processing {len(predictions_LLM)} questions\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Format and Save Predictions\n",
        "\n",
        "Format predictions in the required CSV format and save to file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Format predictions\n",
        "df_prediction = pd.DataFrame(list(predictions_LLM.items()), columns=['id', 'prediction'])\n",
        "df_prediction = df_prediction.sort_values('id')\n",
        "\n",
        "# Format predictions as JSON arrays (required format)\n",
        "df_prediction[\"prediction\"] = df_prediction[\"prediction\"].apply(\n",
        "    lambda x: json.dumps([x], ensure_ascii=False)\n",
        ")\n",
        "\n",
        "# Save to CSV\n",
        "df_prediction.to_csv(PREDICTIONS_CSV, index=False)\n",
        "print(f\"Predictions saved to {PREDICTIONS_CSV}\")\n",
        "print(f\"Total predictions: {len(df_prediction)}\")\n",
        "\n",
        "# Display sample\n",
        "print(\"\\nSample predictions:\")\n",
        "print(df_prediction.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. (Optional) Evaluate on Training Set\n",
        "\n",
        "Evaluate the system on the training set to compute F1 score and compare with baseline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to evaluate on training set\n",
        "# This is useful for parameter tuning\n",
        "\n",
        "# predictions_train = {}\n",
        "# print(f\"Processing {len(df_train)} training questions for evaluation...\")\n",
        "# \n",
        "# for index, row in tqdm(df_train.iterrows(), total=len(df_train), desc=\"Processing training questions\"):\n",
        "#     question = row['question']\n",
        "#     qid = row['id']\n",
        "#     answer = llm_answer(question)\n",
        "#     predictions_train[qid] = answer\n",
        "# \n",
        "# # Format predictions\n",
        "# df_pred_train = pd.DataFrame(list(predictions_train.items()), columns=['id', 'prediction'])\n",
        "# df_pred_train = df_pred_train.sort_values('id')\n",
        "# df_pred_train[\"prediction\"] = df_pred_train[\"prediction\"].apply(\n",
        "#     lambda x: json.dumps([x], ensure_ascii=False)\n",
        "# )\n",
        "# \n",
        "# # Format ground truth\n",
        "# df_gold = df_train.copy()\n",
        "# df_gold[\"answers\"] = df_gold[\"answers\"].apply(lambda x: json.dumps(x, ensure_ascii=False))\n",
        "# \n",
        "# # Evaluate\n",
        "# f1 = score(df_gold, df_pred_train)\n",
        "# print(f\"\\nF1 Score on training set: {f1:.2f}\")\n",
        "# print(f\"Baseline F1: 11.62\")\n",
        "# print(f\"Improvement: {f1 - 11.62:.2f} points\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
