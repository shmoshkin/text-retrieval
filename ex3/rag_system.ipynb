{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Google Colab Setup\n",
        "\n",
        "**If running on Google Colab, run this cell first!**\n",
        "\n",
        "This cell will:\n",
        "1. Install required packages\n",
        "2. Mount Google Drive (if your data files are in Drive)\n",
        "3. Set up the environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if running on Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"‚úÖ Running on Google Colab\")\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    print(\"Running locally\")\n",
        "\n",
        "if IN_COLAB:\n",
        "    # Install dependencies (using subprocess to avoid linter issues)\n",
        "    import subprocess\n",
        "    import sys\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \n",
        "                          \"torch\", \"transformers\", \"huggingface_hub\", \"pandas\", \"tqdm\", \"pyserini\"])\n",
        "    \n",
        "    # Mount Google Drive (optional - only if files are in Drive)\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"‚úÖ Google Drive mounted\")\n",
        "    print(\"üìÅ If your files are in Drive, update DATA_DIR path in the config cell below\")\n",
        "    print(\"üìÅ Otherwise, upload files directly to Colab using the file browser (left sidebar)\")\n",
        "else:\n",
        "    print(\"Running locally - skipping Colab setup\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG System for Question Answering\n",
        "\n",
        "This notebook implements a Retrieval-Augmented Generation (RAG) system that:\n",
        "1. Retrieves relevant Wikipedia passages using Pyserini\n",
        "2. Uses retrieved context with Llama-3.2-1B-Instruct to generate answers\n",
        "3. Processes all test questions and generates predictions\n",
        "\n",
        "**Note**: This is a new implementation based on the template, with all bugs fixed and optimizations applied.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import transformers\n",
        "from huggingface_hub import login\n",
        "from pyserini.search import SimpleSearcher\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. HuggingFace Authentication\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# HuggingFace token (can be set as environment variable KAGGLE_API_TOKEN)\n",
        "hugging_face_token = os.getenv(\"KAGGLE_API_TOKEN\", \"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\")\n",
        "login(hugging_face_token)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Configuration Parameters\n",
        "\n",
        "Adjust these parameters to optimize performance:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Retrieval parameters\n",
        "K = 10  # Number of passages to retrieve\n",
        "RETRIEVAL_METHOD = \"qld\"  # Options: \"qld\" (primary, from course) or \"bm25\" (optional)\n",
        "QLD_MU = 1000  # Dirichlet smoothing parameter for QLD\n",
        "BM25_K1 = 0.9  # BM25 k1 parameter\n",
        "BM25_B = 0.4  # BM25 b parameter\n",
        "CONTEXT_LENGTH = 800  # Max characters per passage (0 = no limit)\n",
        "\n",
        "# LLM parameters\n",
        "MODEL_ID = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "MAX_NEW_TOKENS = 256\n",
        "TEMPERATURE = 0.6\n",
        "TOP_P = 0.9\n",
        "DO_SAMPLE = True\n",
        "\n",
        "# Data paths\n",
        "DATA_DIR = Path(\"ex3/data\")\n",
        "TRAIN_CSV = DATA_DIR / \"train.csv\"\n",
        "TEST_CSV = DATA_DIR / \"test.csv\"\n",
        "PREDICTIONS_CSV = Path(\"ex3/predictions.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load Wikipedia Index\n",
        "\n",
        "Load the pre-built Wikipedia KILT index for retrieval.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyserini.search import SimpleSearcher\n",
        "from pyserini.index.lucene import IndexReader\n",
        "\n",
        "# Initialize searcher with Wikipedia KILT index\n",
        "searcher = SimpleSearcher.from_prebuilt_index('wikipedia-kilt-doc')\n",
        "print(\"Searcher initialized successfully\")\n",
        "\n",
        "# Display index statistics\n",
        "index_reader = IndexReader.from_prebuilt_index('wikipedia-kilt-doc')\n",
        "print(\"\\nIndex Statistics:\")\n",
        "print(index_reader.stats())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Load Data\n",
        "\n",
        "Load training and test datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load training data (for validation/evaluation)\n",
        "df_train = pd.read_csv(TRAIN_CSV, converters={\"answers\": json.loads})\n",
        "print(f\"Loaded {len(df_train)} training questions\")\n",
        "\n",
        "# Load test data (for final predictions)\n",
        "df_test = pd.read_csv(TEST_CSV)\n",
        "print(f\"Loaded {len(df_test)} test questions\")\n",
        "\n",
        "# Display sample\n",
        "print(\"\\nSample training question:\")\n",
        "print(df_train.head(1))\n",
        "print(\"\\nSample test question:\")\n",
        "print(df_test.head(1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Load LLM Model\n",
        "\n",
        "Load Llama-3.2-1B-Instruct model for answer generation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Loading LLM model...\")\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=MODEL_ID,\n",
        "    model_kwargs={\"torch_dtype\": torch.bfloat16, \"device_map\": \"auto\"},\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "print(\"LLM model loaded successfully\")\n",
        "\n",
        "# Get terminators for generation\n",
        "terminators = [\n",
        "    pipeline.tokenizer.eos_token_id,\n",
        "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Retrieval Functions\n",
        "\n",
        "Functions for retrieving relevant passages using different ranking methods.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_context_qld(searcher, query, k, mu=1000):\n",
        "    \"\"\"Retrieve context using Query Likelihood Dirichlet (QLD) method.\"\"\"\n",
        "    searcher.set_qld(mu=mu)\n",
        "    hits = searcher.search(query, k)\n",
        "    return hits\n",
        "\n",
        "def get_context_bm25(searcher, query, k, k1=0.9, b=0.4):\n",
        "    \"\"\"Retrieve context using BM25 method.\"\"\"\n",
        "    searcher.set_bm25(k1=k1, b=b)\n",
        "    hits = searcher.search(query, k)\n",
        "    return hits\n",
        "\n",
        "# Note: Hybrid QLD+BM25 removed - not covered in course material\n",
        "# Focus on QLD (primary method from course) and BM25 (optional alternative) separately\n",
        "\n",
        "def get_context(searcher, query, k=10, retrieval_method=\"qld\"):\n",
        "    \"\"\"\n",
        "    Retrieve relevant passages from Wikipedia index.\n",
        "    Fixed: Uses full passage content instead of truncated snippets.\n",
        "    \"\"\"\n",
        "    # Retrieve hits based on method\n",
        "    if retrieval_method == \"qld\":\n",
        "        hits = get_context_qld(searcher, query, k, mu=QLD_MU)\n",
        "    elif retrieval_method == \"bm25\":\n",
        "        hits = get_context_bm25(searcher, query, k, k1=BM25_K1, b=BM25_B)\n",
        "    # Note: Only QLD (primary) and BM25 (optional) are supported\n",
        "    # Hybrid combination not covered in course material\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown retrieval method: {retrieval_method}. Use 'qld' (primary) or 'bm25' (optional)\")\n",
        "    \n",
        "    # Extract passage text\n",
        "    contexts = []\n",
        "    for hit in hits:\n",
        "        try:\n",
        "            doc = searcher.doc(hit.docid)\n",
        "            raw_json = doc.raw()\n",
        "            data = json.loads(raw_json)\n",
        "            contents = data['contents']\n",
        "            \n",
        "            # Clean and truncate if needed\n",
        "            content = contents.replace('\\n', ' ')\n",
        "            if CONTEXT_LENGTH > 0 and len(content) > CONTEXT_LENGTH:\n",
        "                content = content[:CONTEXT_LENGTH] + \"...\"\n",
        "            \n",
        "            contexts.append(content)\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not retrieve document {hit.docid}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    return contexts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_message(query, contexts):\n",
        "    \"\"\"\n",
        "    Create prompt messages for LLM.\n",
        "    \n",
        "    Fixed bug: uses 'query' parameter instead of undefined 'question' variable.\n",
        "    Improved prompt for better answer extraction.\n",
        "    \"\"\"\n",
        "    # Format contexts\n",
        "    context_text = '\\n\\n'.join([f\"Passage {i+1}: {ctx}\" for i, ctx in enumerate(contexts)])\n",
        "    \n",
        "    system_prompt = \"\"\"You are a question-answering assistant. Your task is to provide concise, accurate answers based ONLY on the information provided in the passages below. \n",
        "\n",
        "Rules:\n",
        "1. Use ONLY information from the provided passages\n",
        "2. Provide a SHORT, DIRECT answer (typically 1-5 words)\n",
        "3. Do NOT include explanations, citations, or additional context\n",
        "4. If the answer is not in the passages, respond with \"I don't know\"\n",
        "5. Extract the answer directly - do not paraphrase unnecessarily\"\"\"\n",
        "\n",
        "    user_prompt = f\"\"\"Based on the following passages, provide a concise answer to the question.\n",
        "\n",
        "Passages:\n",
        "{context_text}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "    ]\n",
        "    \n",
        "    return messages\n",
        "\n",
        "def extract_answer(text):\n",
        "    \"\"\"Extract clean answer from LLM output.\"\"\"\n",
        "    if not text:\n",
        "        return \"I don't know\"\n",
        "    \n",
        "    text = text.strip()\n",
        "    sentences = text.split('.')\n",
        "    if sentences:\n",
        "        first_sentence = sentences[0].strip()\n",
        "        first_sentence = re.sub(r'^(The answer is|Answer:|The answer:|It is|It\\'s)', '', first_sentence, flags=re.IGNORECASE)\n",
        "        first_sentence = first_sentence.strip()\n",
        "        \n",
        "        if len(first_sentence.split()) <= 10:\n",
        "            return first_sentence\n",
        "    \n",
        "    return text[:50].strip()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def llm_answer(query):\n",
        "    \"\"\"\n",
        "    Generate answer using RAG pipeline.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Retrieve context\n",
        "        contexts = get_context(searcher, query, k=K, retrieval_method=RETRIEVAL_METHOD)\n",
        "        \n",
        "        if not contexts:\n",
        "            return \"I don't know\"\n",
        "        \n",
        "        # Create prompt\n",
        "        messages = create_message(query, contexts)\n",
        "        \n",
        "        # Generate answer\n",
        "        outputs = pipeline(\n",
        "            messages,\n",
        "            max_new_tokens=MAX_NEW_TOKENS,\n",
        "            eos_token_id=terminators,\n",
        "            do_sample=DO_SAMPLE,\n",
        "            temperature=TEMPERATURE,\n",
        "            top_p=TOP_P,\n",
        "        )\n",
        "        \n",
        "        # Extract answer\n",
        "        generated_text = outputs[0][\"generated_text\"][-1].get('content', '')\n",
        "        answer = extract_answer(generated_text)\n",
        "        \n",
        "        return answer\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error generating answer for query '{query}': {e}\")\n",
        "        return \"I don't know\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Evaluation Functions\n",
        "\n",
        "Functions for evaluating predictions using F1 score.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_answer(s):\n",
        "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        return ''.join(ch for ch in text if ch not in set(string.punctuation))\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "    \"\"\"Compute token-level F1 score between prediction and a ground truth.\"\"\"\n",
        "    pred_tokens = normalize_answer(prediction).split()\n",
        "    gt_tokens = normalize_answer(ground_truth).split()\n",
        "    common = Counter(pred_tokens) & Counter(gt_tokens)\n",
        "    num_same = sum(common.values())\n",
        "\n",
        "    if len(pred_tokens) == 0 or len(gt_tokens) == 0:\n",
        "        return int(pred_tokens == gt_tokens)\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "\n",
        "    precision = num_same / len(pred_tokens)\n",
        "    recall = num_same / len(gt_tokens)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
        "    return max(metric_fn(prediction, gt) for gt in ground_truths)\n",
        "\n",
        "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str = 'id') -> float:\n",
        "    \"\"\"Computes average F1 score over all questions.\"\"\"\n",
        "    gold = solution.set_index(row_id_column_name)\n",
        "    pred = submission.set_index(row_id_column_name)\n",
        "\n",
        "    f1_sum = 0.0\n",
        "    count = 0\n",
        "\n",
        "    for qid in gold.index:\n",
        "        if qid not in pred.index:\n",
        "            print(f\"Missing prediction for question ID: {qid}\")\n",
        "            count += 1\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            ground_truths = json.loads(gold.loc[qid, \"answers\"])\n",
        "            if not isinstance(ground_truths, list):\n",
        "                raise ValueError\n",
        "        except Exception:\n",
        "            raise Exception(f\"Invalid format for answers at id {qid}: must be a JSON list of strings.\")\n",
        "\n",
        "        prediction = pred.loc[qid, \"prediction\"]\n",
        "        f1 = metric_max_over_ground_truths(f1_score, prediction, ground_truths)\n",
        "\n",
        "        f1_sum += f1\n",
        "        count += 1\n",
        "\n",
        "    if count == 0:\n",
        "        raise Exception(\"No matching question IDs between submission and solution.\")\n",
        "\n",
        "    return 100.0 * f1_sum / count\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Process Test Questions\n",
        "\n",
        "Process all test questions and generate predictions. **Fixed**: Processes all 2032 questions instead of just 5.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process all test questions\n",
        "predictions_LLM = {}\n",
        "\n",
        "print(f\"Processing {len(df_test)} test questions...\")\n",
        "print(f\"Retrieval method: {RETRIEVAL_METHOD}, k={K}\")\n",
        "\n",
        "for index, row in tqdm(df_test.iterrows(), total=len(df_test), desc=\"Processing questions\"):\n",
        "    question = row['question']\n",
        "    qid = row['id']\n",
        "    \n",
        "    answer = llm_answer(question)\n",
        "    predictions_LLM[qid] = answer\n",
        "\n",
        "print(f\"\\nCompleted processing {len(predictions_LLM)} questions\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Format and Save Predictions\n",
        "\n",
        "Format predictions in the required CSV format and save to file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Format predictions\n",
        "df_prediction = pd.DataFrame(list(predictions_LLM.items()), columns=['id', 'prediction'])\n",
        "df_prediction = df_prediction.sort_values('id')\n",
        "\n",
        "# Format predictions as JSON arrays (required format)\n",
        "df_prediction[\"prediction\"] = df_prediction[\"prediction\"].apply(\n",
        "    lambda x: json.dumps([x], ensure_ascii=False)\n",
        ")\n",
        "\n",
        "# Save to CSV\n",
        "df_prediction.to_csv(PREDICTIONS_CSV, index=False)\n",
        "print(f\"Predictions saved to {PREDICTIONS_CSV}\")\n",
        "print(f\"Total predictions: {len(df_prediction)}\")\n",
        "\n",
        "# Display sample\n",
        "print(\"\\nSample predictions:\")\n",
        "print(df_prediction.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. (Optional) Evaluate on Training Set\n",
        "\n",
        "Evaluate the system on the training set to compute F1 score and compare with baseline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to evaluate on training set\n",
        "# This is useful for parameter tuning\n",
        "\n",
        "# predictions_train = {}\n",
        "# print(f\"Processing {len(df_train)} training questions for evaluation...\")\n",
        "# \n",
        "# for index, row in tqdm(df_train.iterrows(), total=len(df_train), desc=\"Processing training questions\"):\n",
        "#     question = row['question']\n",
        "#     qid = row['id']\n",
        "#     answer = llm_answer(question)\n",
        "#     predictions_train[qid] = answer\n",
        "# \n",
        "# # Format predictions\n",
        "# df_pred_train = pd.DataFrame(list(predictions_train.items()), columns=['id', 'prediction'])\n",
        "# df_pred_train = df_pred_train.sort_values('id')\n",
        "# df_pred_train[\"prediction\"] = df_pred_train[\"prediction\"].apply(\n",
        "#     lambda x: json.dumps([x], ensure_ascii=False)\n",
        "# )\n",
        "# \n",
        "# # Format ground truth\n",
        "# df_gold = df_train.copy()\n",
        "# df_gold[\"answers\"] = df_gold[\"answers\"].apply(lambda x: json.dumps(x, ensure_ascii=False))\n",
        "# \n",
        "# # Evaluate\n",
        "# f1 = score(df_gold, df_pred_train)\n",
        "# print(f\"\\nF1 Score on training set: {f1:.2f}\")\n",
        "# print(f\"Baseline F1: 11.62\")\n",
        "# print(f\"Improvement: {f1 - 11.62:.2f} points\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
